{
 "metadata": {
  "name": "",
  "signature": "sha256:64ba76f094e74eb9cf457bc35b0a28006976596cb2fb133a3d93f24ac5330a66"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import IPython\n",
      "import sklearn as sk\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['clf', 'split']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#run main.sh at the start to get the data from Charlie + do some preprocessing\n",
      "from glob import glob\n",
      "import os\n",
      "CORPUS_LOCATION = \"/Volumes/usb/\" \n",
      "contracts = glob(CORPUS_LOCATION + \"*_text*\") \n",
      "labeled_data = [l.replace(\"\\n\", \"\") for l in open(\"labels.csv\")]\n",
      "labeled_data = [(l.split(\",\")[0],l.split(\",\")[1])  for l in labeled_data]\n",
      "\n",
      "def get_contract_text(label):\n",
      "    contract_lines = [l.replace(\"\\\\n\", \"\") for l in open(label)]\n",
      "    contract_lines = \" \".join(contract_lines)\n",
      "    return contract_lines\n",
      "\n",
      "def exists_in_corpus(label):\n",
      "    return os.path.exists(CORPUS_LOCATION + label + \"_text.txt\")\n",
      "\n",
      "all_texts = [get_contract_text(c) for c in contracts]\n",
      "print all_texts[0]\n",
      "labeled_data = [l for l in labeled_data if exists_in_corpus(l[0])]\n",
      "labels = [l[1] for l in labeled_data]\n",
      "texts = [get_contract_text(l[0]) for l in labeled_data]\n",
      "contract_nos = [l[0] for l in labeled_data]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: '/Volumes/usb//Volumes/usb/1019447-downtown-development-district-contract-with-city_text.txt_text.txt'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-129-32346320438e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCORPUS_LOCATION\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_text.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mall_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_contract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontracts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mall_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlabeled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexists_in_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-129-32346320438e>\u001b[0m in \u001b[0;36mget_contract_text\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_contract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcontract_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCORPUS_LOCATION\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_text.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcontract_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontract_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontract_lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/usb//Volumes/usb/1019447-downtown-development-district-contract-with-city_text.txt_text.txt'"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score, KFold\n",
      "from scipy.stats import sem\n",
      "\n",
      "def evaluate_cross_validation(clf, X, y, K):\n",
      "    # create a k-fold croos validation iterator of k=5 folds\n",
      "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
      "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
      "    scores = cross_val_score(clf, X, y, cv=cv)\n",
      "    print scores\n",
      "    print (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
      "        np.mean(scores), sem(scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import tree\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
      "\n",
      "stop_words = [w for w in nltk.corpus.stopwords.words('english')]\n",
      "stop_words.append(\"New\")\n",
      "stop_words.append(\"Orleans\")\n",
      "stop_words.append(\"City\")\n",
      "\n",
      "\n",
      "stop_words = set([w for w in nltk.corpus.stopwords.words('english')])\n",
      "\n",
      "clf_1 = Pipeline([\n",
      "    ('vect', CountVectorizer()),\n",
      "    ('clf', MultinomialNB()),\n",
      "])\n",
      "clf_2 = Pipeline([\n",
      "    ('vect', HashingVectorizer(non_negative=True)),\n",
      "    ('clf', MultinomialNB()),\n",
      "])\n",
      "clf_3 = Pipeline([\n",
      "    ('vect', TfidfVectorizer()),\n",
      "    ('clf', MultinomialNB()),\n",
      "])\n",
      "clf_4 = Pipeline([\n",
      "    ('vect', TfidfVectorizer(\n",
      "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
      "    )),\n",
      "    ('clf', MultinomialNB()),\n",
      "])\n",
      "clf_5 = Pipeline([\n",
      "    ('vect', CountVectorizer(\n",
      "                stop_words=stop_words,\n",
      "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
      "    )),\n",
      "    ('clf', MultinomialNB()),\n",
      "])\n",
      "clf_6 = Pipeline([\n",
      "    ('vect', CountVectorizer(\n",
      "                stop_words=stop_words,\n",
      "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
      "    )),\n",
      "    ('clf', MultinomialNB(alpha=0.01)),\n",
      "])\n",
      "clf_7 = Pipeline([\n",
      "    ('vect', CountVectorizer(\n",
      "                stop_words=stop_words,\n",
      "                token_pattern=ur\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
      "    )),\n",
      "    ('clf', MultinomialNB(alpha=0.03)),\n",
      "])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clfs = [clf_1, clf_2, clf_3, clf_4, clf_5, clf_6, clf_7]\n",
      "for clf in clfs:\n",
      "    evaluate_cross_validation(clf, texts, labels, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.75555556  0.71375465]\n",
        "Mean score: 0.735 (+/-0.021)\n",
        "[ 0.51481481  0.72862454]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean score: 0.622 (+/-0.107)\n",
        "[ 0.73333333  0.66542751]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean score: 0.699 (+/-0.034)\n",
        "[ 0.74814815  0.67657993]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean score: 0.712 (+/-0.036)\n",
        "[ 0.78148148  0.75464684]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean score: 0.768 (+/-0.013)\n",
        "[ 0.81481481  0.85501859]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean score: 0.835 (+/-0.020)\n",
        "[ 0.81481481  0.85501859]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean score: 0.835 (+/-0.020)\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_7.fit(texts, labels)\n",
      "y_pred = clf_7.predict(X_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 128,
       "text": [
        "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), prep...nizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=0.03, class_prior=None, fit_prior=True))])"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}